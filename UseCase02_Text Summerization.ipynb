{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNS28ESVWl0KdWGEtDaxRa6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text summarization using the 'transformers' library\n","\n","---\n","\n"],"metadata":{"id":"8Ezk8UqrRICj"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"qTSBYvhGQpAU","executionInfo":{"status":"ok","timestamp":1723612566077,"user_tz":-60,"elapsed":269,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}}},"outputs":[],"source":["import transformers\n","\n","# Suppress warning messages from the transformers library to keep the output clean.\n","transformers.logging.set_verbosity_error()"]},{"cell_type":"code","source":["# Define a verbose text containing information about Earth.\n","verbose_text = \"\"\"\n","Earth is the third planet from the Sun and the only astronomical object\n","known to harbor life.\n","While large volumes of water can be found\n","throughout the Solar System, only Earth sustains liquid surface water.\n","About 71% of Earth's surface is made up of the ocean, dwarfing\n","Earth's polar ice, lakes, and rivers.\n","The remaining 29% of Earth's\n","surface is land, consisting of continents and islands.\n","Earth's surface layer is formed of several slowly moving tectonic plates,\n","interacting to produce mountain ranges, volcanoes, and earthquakes.\n","Earth's liquid outer core generates the magnetic field that shapes Earth's\n","magnetosphere, deflecting destructive solar winds.\n","\"\"\"\n","\n","# Remove newline characters from the text to prepare it for summarization.\n","verbose_text = verbose_text.replace(\"\\n\", \"\")\n"],"metadata":{"id":"J16xuNSzRFL-","executionInfo":{"status":"ok","timestamp":1723612575969,"user_tz":-60,"elapsed":252,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","# Import the pipeline function from the transformers library.\n","from transformers import pipeline\n","\n","# Initialize the summarization pipeline, which generates a concise summary\n","# of the provided text. The summary will be between 10 and 100 words long.\n","summarizer = pipeline(\"summarization\",\n","                      min_length=10,\n","                      max_length=100)\n","\n","# Generate a summary of the verbose text.\n","summary = summarizer(verbose_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NiAO_-iRdj1","executionInfo":{"status":"ok","timestamp":1723612603260,"user_tz":-60,"elapsed":15039,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"71393dcf-5b4b-4561-8919-5789367bd7fa"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["\n","# Extract and print the summary text from the result.\n","print(summary[0].get(\"summary_text\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O0BTFFopSRGN","executionInfo":{"status":"ok","timestamp":1723612614417,"user_tz":-60,"elapsed":253,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"3f5926a3-9d72-4e6e-bfb2-6009ce9b52fc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":[" Earth is the third planet from the Sun and the only astronomical object known to harbor life . About 71% of Earth's surface is made up of the ocean, dwarfing Earth's polar ice, lakes, and rivers . The remaining 29% of the surface is land, consisting of continents and islands .\n"]}]},{"cell_type":"code","source":["print(\"Checkpoint used: \", summarizer.model.config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cap56xeRSKce","executionInfo":{"status":"ok","timestamp":1723612629060,"user_tz":-60,"elapsed":266,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"776092a0-a4ec-422f-f3f9-7394a88a583a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint used:  BartConfig {\n","  \"_name_or_path\": \"sshleifer/distilbart-cnn-12-6\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"replacing_rate\": 0,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"student_decoder_layers\": null,\n","  \"student_encoder_layers\": null,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"transformers_version\": \"4.42.4\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n"]}]},{"cell_type":"markdown","source":["## Evaluation Using ROUGE metric\n"],"metadata":{"id":"FF8jJM1NSm3q"}},{"cell_type":"markdown","source":["### Using single text\n"],"metadata":{"id":"kIemtN8bWUlS"}},{"cell_type":"code","source":["!pip install rouge_score\n","!pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_h0_5rZmTQtM","executionInfo":{"status":"ok","timestamp":1723612648784,"user_tz":-60,"elapsed":14218,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"2d3a1115-99d3-4308-d257-db0ac3acd21f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.20.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.3.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"]}]},{"cell_type":"code","source":["import evaluate\n","\n","# Load the ROUGE evaluation metric using the evaluate library.\n","rouge_evaluator = evaluate.load(\"rouge\")\n","\n","# Define the reference and prediction texts for evaluation.\n","reference_text = [\"This is the same string\"]\n","predict_text = [\"This is the same string\"]\n","\n","# Compute the ROUGE scores by comparing the prediction with the reference text.\n","eval_results = rouge_evaluator.compute(predictions=predict_text,\n","                                       references=reference_text)\n","\n","# Display the ROUGE evaluation results for the exact match scenario.\n","print(\"Results for Exact Match:\", eval_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-URIBCGESu9P","executionInfo":{"status":"ok","timestamp":1723612678964,"user_tz":-60,"elapsed":1054,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"1d74de60-e02c-46ad-f7d9-0f153ffa7b07"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Results for Exact Match: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n"]}]},{"cell_type":"code","source":["# Define the reference and predicted texts for evaluation, which do not match.\n","reference_text = [\"This is the different string\"]\n","predict_text = [\"Google can predict warm weather\"]\n","\n","# Compute the ROUGE scores to assess the similarity between the reference and predicted texts.\n","eval_results = rouge_evaluator.compute(predictions=predict_text,\n","                                       references=reference_text)\n","\n","# Print the evaluation results for the case where there is no match between reference and predicted texts.\n","print(\"\\nEvaluation Results for No Match:\", eval_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-i88EvsTeAt","executionInfo":{"status":"ok","timestamp":1723612709225,"user_tz":-60,"elapsed":1059,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"6626adac-c87a-42f3-c0ab-820cfe5ca792"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Results for No Match: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-MQPPHppVKT1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"2hMk4l3qVfnV"}},{"cell_type":"markdown","source":["Observation:\n","\n","Computing ROUGE Scores:\n","\n","    The rouge_evaluator.compute method compares the predicted text against the reference text using ROUGE metrics. The key ROUGE scores are:\n","\n","      - ROUGE-1: Measures the overlap of unigrams (single words).\n","      - ROUGE-2: Measures the overlap of bigrams (two-word sequences).\n","      - ROUGE-L: Measures the longest common subsequence.\n","\n","    Given that the reference and prediction texts are different, the ROUGE scores will likely be close to zero, indicating minimal overlap.\n","\n","\n","Explanation of Results\n","\n","  - ROUGE-1 Score: A score of 0.0 indicates no overlap between unigrams (words) in the reference and prediction.\n","  - ROUGE-2 Score: A score of 0.0 indicates no overlap between bigrams (two-word sequences) in the reference and prediction.\n","  - ROUGE-L Score: A score of 0.0 indicates no common subsequences between the reference and prediction.\n","  - ROUGE-Lsum Score: Similar to ROUGE-L but for summarization tasks, also showing 0.0 due to no overlap."],"metadata":{"id":"w3WEagW5VePg"}},{"cell_type":"markdown","source":["### Using Multiple Texts"],"metadata":{"id":"x83BBv0FWawe"}},{"cell_type":"code","source":["import evaluate\n","\n","# Load the ROUGE evaluation metric using the evaluate library.\n","rouge_evaluator = evaluate.load(\"rouge\")\n","\n","# Define lists of reference texts and corresponding predicted texts for evaluation.\n","reference_texts = [\n","    \"Earth is the third planet from the Sun and the only astronomical object known to harbor life.\",\n","    \"The Solar System contains large volumes of water, but only Earth has liquid surface water.\"\n","]\n","predict_texts = [\n","    \"Earth is the third planet from the Sun and is known to harbor life.\",\n","    \"Earth is the only planet with liquid surface water.\"\n","]\n","\n","# Compute the ROUGE scores by comparing the lists of predictions with the lists of reference texts.\n","eval_results = rouge_evaluator.compute(predictions=predict_texts,\n","                                       references=reference_texts)\n","\n","# Display the ROUGE evaluation results for the given texts.\n","print(\"Results for Multiple Texts:\", eval_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCOdxUFHWRJD","executionInfo":{"status":"ok","timestamp":1723613230628,"user_tz":-60,"elapsed":1075,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"993bbb97-9e8f-4b63-acc7-5d7cfd0a8634"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Results for Multiple Texts: {'rouge1': 0.6693548387096775, 'rouge2': 0.4702194357366771, 'rougeL': 0.6276881720430108, 'rougeLsum': 0.6276881720430108}\n"]}]},{"cell_type":"markdown","source":["### Observations:\n","\n","1. **ROUGE-1 Score (0.669):**\n","   - **Meaning:** ROUGE-1 measures the overlap of unigrams (single words) between the predicted and reference texts. A score of 0.669 indicates a moderate level of overlap in individual words.\n","   - **Interpretation:** The predictions have a good amount of overlap with the reference texts in terms of individual words, but there is still room for improvement. This score reflects a reasonable level of accuracy in capturing the main terms and concepts.\n","\n","2. **ROUGE-2 Score (0.470):**\n","   - **Meaning:** ROUGE-2 assesses the overlap of bigrams (pairs of consecutive words) between the prediction and the reference. A score of 0.470 suggests a lower overlap of bigrams compared to unigrams.\n","   - **Interpretation:** This indicates that while the individual words are reasonably well-matched, the connections between pairs of words are less well aligned. This suggests that the predictions may have some gaps in capturing the specific phrases or sequences present in the references.\n","\n","3. **ROUGE-L Score (0.628):**\n","   - **Meaning:** ROUGE-L evaluates the longest common subsequence between the reference and the prediction texts. A score of 0.628 reflects a moderate overlap in longer sequences of words.\n","   - **Interpretation:** This score indicates a decent match in terms of the longer, meaningful sequences of words. The predictions align reasonably well with the reference texts in terms of maintaining the structure and coherence of longer text spans.\n","\n","4. **ROUGE-Lsum Score (0.628):**\n","   - **Meaning:** ROUGE-Lsum is similar to ROUGE-L but is used in the context of summarization tasks. It measures the overlap of longest common subsequences in the context of summaries.\n","   - **Interpretation:** The score matches the ROUGE-L score, suggesting consistency in evaluating the longest common subsequences within the summaries. This reflects a reasonable level of alignment in summarizing the key content of the reference texts.\n","\n","### Summary of Observations:\n","\n","- **Overall Alignment:** The ROUGE-1 and ROUGE-L scores are relatively high, indicating that the predictions align well with the reference texts in terms of individual words and longer sequences. However, ROUGE-2 shows a lower score, suggesting that the predictions may not capture all the specific bigrams or key phrases as effectively.\n","\n","- **Improvement Areas:** The lower ROUGE-2 score highlights that there is room for improvement in capturing more specific sequences of words or phrases. Enhancing the ability to match bigrams can lead to a more precise alignment with the reference texts.\n","\n","- **Consistency:** The ROUGE-L and ROUGE-Lsum scores are consistent, reflecting that the summarization aligns well in terms of the longer sequences of text. This indicates that the predictions maintain coherence and key content relatively well.\n"],"metadata":{"id":"78kc8Oe4XIbG"}},{"cell_type":"code","source":[],"metadata":{"id":"crk8wyNbWrCS"},"execution_count":null,"outputs":[]}]}